import h5py
import cv2
import numpy as np
import sys, os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from utils import DINO_features, load_pretrained_DINO, find_bbox
from UnionCut.DINOinference import TokenGraphCut
from multiprocessing import Manager, Process

def h5writer(result_queue):
    # load dino
    net = load_pretrained_DINO(path="/media/wzl/T7/PhysicalImageAugmentation/LOST/dino_deitsmall8_pretrain.pth")
    # create the h5 file
    file = h5py.File("./TokenGraphCutN5_VOC.h5", "w")
    # create a group to save pseudo-labels generated by the current method
    method_group = file.create_group("TokenGraphCut")
    print("SAVER is ready")
    # save results one by one
    while True:
        result = result_queue.get()
        if result == "end":
            # finish when the mask generation is finished
            break
        try:
            # obtain info of the image
            image, image_name, local_path, image_path, masks = result
            # save path of the image
            method_group.create_dataset(image_name + '/path', data=image_path)
            # save the mask
            method_group.create_dataset(image_name + '/masks', data=masks, compression="gzip", compression_opts=9)
            # obtain features of the image
            features, qs, ks, vs = DINO_features(net, image)
            features = features.squeeze(0)[1:, :].permute(1, 0).reshape(-1, 28, 28).cpu().numpy()  # 384, 28, 28
            # combine features from multi-head, and remove CLS token
            ks = ks.permute(0, 2, 1, 3).reshape(1, 785, -1).squeeze(0)[1:, :].permute(1, 0).reshape(-1, 28,
                                                                                                    28).cpu().numpy()  # 384, 28, 28
            qs = qs.permute(0, 2, 1, 3).reshape(1, 785, -1).squeeze(0)[1:, :].permute(1, 0).reshape(-1, 28,
                                                                                                    28).cpu().numpy()  # 384, 28, 28
            vs = vs.permute(0, 2, 1, 3).reshape(1, 785, -1).squeeze(0)[1:, :].permute(1, 0).reshape(-1, 28,
                                                                                                    28).cpu().numpy()  # 384, 28, 28
            # obtain feature vector of each mask
            for i in range(masks.shape[0]):
                # obtain current mask
                mask = masks[i]
                # obtain cls feature vectors in the masked area
                # crop the masked area first
                [xmin, ymin, xmax, ymax] = find_bbox(mask, mass_center=False)
                cropped_image = image[ymin:ymax + 1, xmin:xmax + 1]
                # obtain features of the cropped image
                cropped_features, cropped_qs, cropped_ks, cropped_vs = DINO_features(net, cropped_image)
                # obtain features of the CLS token
                cls_feature = cropped_features.squeeze(0)[0, :].cpu().numpy()
                cls_q = cropped_qs.permute(0, 2, 1, 3).reshape(1, 785, -1).squeeze(0)[0, :].cpu().numpy()
                cls_k = cropped_ks.permute(0, 2, 1, 3).reshape(1, 785, -1).squeeze(0)[0, :].cpu().numpy()
                cls_v = cropped_vs.permute(0, 2, 1, 3).reshape(1, 785, -1).squeeze(0)[0, :].cpu().numpy()
                # resize current mask
                mask = cv2.resize(masks[i], (28, 28), cv2.INTER_NEAREST)
                # obtain mean feature vectors in the masked area
                ks_mean = np.mean(ks[:, mask == 1], axis=1)
                qs_mean = np.mean(qs[:, mask == 1], axis=1)
                vs_mean = np.mean(vs[:, mask == 1], axis=1)
                features_mean = np.mean(features[:, mask == 1], axis=1)

                # save features
                method_group.create_dataset(image_name + '/feature_vector/' + str(i) + '/mean/feature',
                                            data=features_mean, compression="gzip", compression_opts=9)
                method_group.create_dataset(image_name + '/feature_vector/' + str(i) + '/mean/k', data=ks_mean,
                                            compression="gzip", compression_opts=9)
                method_group.create_dataset(image_name + '/feature_vector/' + str(i) + '/mean/q', data=qs_mean,
                                            compression="gzip", compression_opts=9)
                method_group.create_dataset(image_name + '/feature_vector/' + str(i) + '/mean/v', data=vs_mean,
                                            compression="gzip", compression_opts=9)
                method_group.create_dataset(image_name + '/feature_vector/' + str(i) + '/cls/feature', data=cls_feature,
                                            compression="gzip", compression_opts=9)
                method_group.create_dataset(image_name + '/feature_vector/' + str(i) + '/cls/k', data=cls_k,
                                            compression="gzip", compression_opts=9)
                method_group.create_dataset(image_name + '/feature_vector/' + str(i) + '/cls/q', data=cls_q,
                                            compression="gzip", compression_opts=9)
                method_group.create_dataset(image_name + '/feature_vector/' + str(i) + '/cls/v', data=cls_v,
                                            compression="gzip", compression_opts=9)
        except:
            continue
    # close the h5 file
    file.close()

def mask_generation(result_queue, image_name, local_path, image_path):
    #try:
    # load the image
    image = cv2.imread(local_path)
    # obtain masks
    masks = TokenGraphCut(image)
    # filter masks with 0 area
    areas = []
    for i in range(masks.shape[0]):
        mask = cv2.resize(masks[i], (28, 28), cv2.INTER_NEAREST)
        areas.append(np.sum(mask))
    areas = np.array(areas)
    if np.sum(areas) == 0:
        return
    masks = masks[areas > 0, :, :]
    # send the result to saver
    result_queue.put((image, image_name, local_path, image_path, masks))
    #except:
        #print("generation no")
        #return

if __name__ == "__main__":
    # obtain paths of each image
    root_path = "/home/wzl/"
    fh = open(root_path + "VOC/VOC2012/VOCtrainval_11-May-2012/VOCdevkit/VOC2012/ImageSets/Segmentation/trainval.txt",
              'r')
    # save names of each image
    image_names = [line.strip('\n') for line in fh]
    # save absolute paths of each image in the computer
    local_paths = [root_path + "VOC/VOC2012/VOCtrainval_11-May-2012/VOCdevkit/VOC2012/JPEGImages/" + image_name + ".jpg"
                   for image_name in image_names]
    # save relevant paths of each image under the dataset
    image_paths = ["VOC2012/JPEGImages/" + image_name + ".jpg" for image_name in image_names]

    manager = Manager()
    result_queue = manager.Queue()

    # create and launch h5writer
    h5writer_process = Process(target=h5writer, args=(result_queue,))
    h5writer_process.start()

    # traverse images
    counter = 0
    # initialize process pool
    processes = []
    # initialize process pool size
    process_num = 6
    while True:
        # remove subprocesses which have finished
        for process in processes[:]:
            if not process.is_alive():
                processes.remove(process)
        # start new sub-process to simulate games
        if len(processes) < process_num and counter < len(image_names):
            for _ in range(process_num - len(processes)):
                # obtain current image
                image_name, local_path, image_path = image_names[counter], local_paths[counter], image_paths[counter]
                # create a new sub-process
                p = Process(target=mask_generation, args=(result_queue, image_name, local_path, image_path,))
                p.start()
                processes.append(p)
                counter += 1
                print(counter)
                if counter == len(image_names):
                    break
        # finish
        if len(processes) == 0 and counter == len(image_names):
            break

    # end h5 writer
    result_queue.put("end")
    h5writer_process.join()
    print("done")
